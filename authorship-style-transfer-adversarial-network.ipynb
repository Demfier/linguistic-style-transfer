{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file_path = \"data/c50-articles.txt\"\n",
    "label_file_path = \"data/c50-labels.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion of texts into integer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=VOCAB_SIZE, filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "with open(text_file_path) as text_file:\n",
    "    text_tokenizer.fit_on_texts(text_file)\n",
    "    \n",
    "with open(text_file_path) as text_file:\n",
    "    integer_text_sequences = text_tokenizer.texts_to_sequences(text_file)\n",
    "\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "     integer_text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "print(padded_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_INDEX = text_tokenizer.word_index['<sos>']\n",
    "EOS_INDEX = text_tokenizer.word_index['<eos>']\n",
    "print(SOS_INDEX, EOS_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion of labels to one-hot represenations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tokenizer =  tf.keras.preprocessing.text.Tokenizer(lower=False)\n",
    "\n",
    "with open(label_file_path) as label_file:\n",
    "    label_tokenizer.fit_on_texts(label_file)\n",
    "\n",
    "with open(label_file_path) as label_file:\n",
    "    label_sequences = label_tokenizer.texts_to_sequences(label_file)\n",
    "\n",
    "NUM_LABELS = len(label_tokenizer.word_index)\n",
    "one_hot_labels = np.asarray(list(\n",
    "    map(lambda x: np.eye(NUM_LABELS, k=x[0])[0], label_sequences)))\n",
    "\n",
    "print(one_hot_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeAdversarialNetwork():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.style_embedding_size = 128\n",
    "        self.content_embedding_size = 128\n",
    "        self.build_model()\n",
    "    \n",
    "    def get_sentence_representation(self, index_sequence, word_embeddings):\n",
    "        \n",
    "        embedded_sequence = tf.nn.embedding_lookup(\n",
    "            word_embeddings, index_sequence, name=\"embedded_sequence\")\n",
    "\n",
    "        lstm_cell_fw = tf.contrib.rnn.BasicLSTMCell(\n",
    "            num_units=128, name=\"lstm_cell_fw_content\")\n",
    "        lstm_cell_bw = tf.contrib.rnn.BasicLSTMCell(\n",
    "            num_units=128, name=\"lstm_cell_bw_content\")\n",
    "\n",
    "        rnn_outputs, rnn_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=lstm_cell_fw, cell_bw=lstm_cell_bw, \n",
    "            inputs=embedded_sequence, \n",
    "            dtype=tf.float32, time_major=False)\n",
    "        rnn_state = tf.concat(\n",
    "            values=[rnn_states[0].h, rnn_states[1].h], axis=1, \n",
    "            name=\"sentence_representation\")\n",
    "\n",
    "        return rnn_state\n",
    "\n",
    "    def get_content_representation(self, sentence_representation):\n",
    "        \n",
    "        dense_content = tf.layers.dense(\n",
    "            inputs=sentence_representation, units=self.content_embedding_size, \n",
    "            activation=tf.nn.relu, name=\"content_representation\")\n",
    "\n",
    "        return dense_content\n",
    "\n",
    "    def get_style_representation(self, sentence_representation):\n",
    "        \n",
    "        dense_style = tf.layers.dense(\n",
    "            inputs=sentence_representation, units=self.style_embedding_size, \n",
    "            activation=tf.nn.relu, name=\"style_representation\")\n",
    "        return dense_style\n",
    "\n",
    "    def get_label_prediction(self, content_representation):\n",
    "\n",
    "        dense_1 = tf.layers.dense(\n",
    "            inputs=content_representation, units=NUM_LABELS, \n",
    "            activation=tf.nn.relu, name=\"dense_1\")\n",
    "        \n",
    "        softmax_output = tf.nn.softmax(dense_1, name=\"label_prediction\")\n",
    "\n",
    "        return softmax_output\n",
    "    \n",
    "    def generate_output_sequence(self, word_embeddings, style_representation, \n",
    "                                 content_representation):\n",
    "        \n",
    "        last_predicted_word_index = SOS_INDEX\n",
    "        indices = tf.ones(shape=[self.batch_size], dtype=tf.int32) * SOS_INDEX\n",
    "        print(\"indices: {}\".format(indices))\n",
    "        \n",
    "        actual_initial_state = tf.one_hot(\n",
    "            indices=indices, depth=VOCAB_SIZE)\n",
    "        print(\"actual_initial_state: {}\".format(actual_initial_state))\n",
    "        \n",
    "        generative_embedding = tf.concat(\n",
    "            values=[style_representation, content_representation], axis=1)\n",
    "        print(\"generative_embedding: {}\".format(generative_embedding))\n",
    "        \n",
    "        repeated_vector = tf.tile(\n",
    "            input=generative_embedding, \n",
    "            multiples=tf.constant([1, MAX_SEQUENCE_LENGTH]), \n",
    "            name='repeated_vector')\n",
    "        print(\"repeated_vector: {}\".format(repeated_vector))\n",
    "        \n",
    "        reshaped_sequence = tf.reshape(\n",
    "            tensor=repeated_vector, \n",
    "            shape=[self.batch_size, MAX_SEQUENCE_LENGTH, \n",
    "                   self.style_embedding_size + self.content_embedding_size],\n",
    "            name='reshaped_sequence'\n",
    "        )\n",
    "        print(\"reshaped_sequence: {}\".format(reshaped_sequence))\n",
    "        \n",
    "        decoder_rnn_cell = tf.nn.rnn_cell.BasicRNNCell(VOCAB_SIZE)\n",
    "        decoder_outputs, _ = tf.nn.dynamic_rnn(\n",
    "            cell=decoder_rnn_cell, inputs=reshaped_sequence, \n",
    "            initial_state=actual_initial_state)\n",
    "        print(\"decoder_outputs: {}\".format(decoder_outputs))\n",
    "        \n",
    "        softmax_prediction = tf.nn.softmax(decoder_outputs)\n",
    "        print(\"softmax_prediction: {}\".format(softmax_prediction))\n",
    "\n",
    "        return softmax_prediction\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        self.input_sequence = tf.placeholder(\n",
    "            dtype=tf.int32, shape=[None, MAX_SEQUENCE_LENGTH], \n",
    "            name=\"input_sequence\")\n",
    "        print(\"input_sequence: {}\".format(self.input_sequence))\n",
    "\n",
    "        self.input_label = tf.placeholder(\n",
    "            dtype=tf.float32, shape=[None, NUM_LABELS], \n",
    "            name=\"input_label\")\n",
    "        print(\"input_label: {}\".format(self.input_label))\n",
    "        \n",
    "        self.batch_size = tf.shape(self.input_sequence)[0]\n",
    "\n",
    "        # learn embeddings matrix\n",
    "        # can be initialized with pre-trained embeddings\n",
    "        word_embeddings = tf.get_variable(\n",
    "            shape=[VOCAB_SIZE + 1, EMBEDDING_SIZE], name=\"word_embeddings\", \n",
    "            dtype=tf.float32)\n",
    "        print(\"word_embeddings: {}\".format(word_embeddings))\n",
    "\n",
    "        # get sentence representation\n",
    "        sentence_representation = self.get_sentence_representation(\n",
    "            self.input_sequence, word_embeddings)\n",
    "        print(\"sentence_representation: {}\".format(sentence_representation))\n",
    "\n",
    "        # get content representation\n",
    "        content_representation = self.get_content_representation(\n",
    "            sentence_representation)\n",
    "        print(\"content_representation: {}\".format(content_representation))\n",
    "\n",
    "        # use content representation to predict a label\n",
    "        self.label_prediction = self.get_label_prediction(\n",
    "            content_representation)\n",
    "        print(\"label_prediction: {}\".format(self.label_prediction))\n",
    "\n",
    "        self.adversarial_loss = tf.losses.softmax_cross_entropy(\n",
    "            onehot_labels=self.input_label, logits=self.label_prediction)\n",
    "        print(\"adversarial_loss: {}\".format(self.adversarial_loss))\n",
    "\n",
    "        self.adversarial_loss_summary = tf.summary.scalar(\n",
    "            tensor=self.adversarial_loss, name=\"adversarial_loss\")\n",
    "\n",
    "        # get style representation\n",
    "        style_representation = self.get_style_representation(\n",
    "            sentence_representation)\n",
    "        print(\"style_representation: {}\".format(style_representation))\n",
    "        \n",
    "        # generate new sentence\n",
    "        self.generated_logits = self.generate_output_sequence(\n",
    "            word_embeddings, style_representation, content_representation)\n",
    "        print(\"generated_logits: {}\".format(self.generated_logits))\n",
    "        \n",
    "        self.reconstruction_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits=self.generated_logits, targets=self.input_sequence, \n",
    "            weights=tf.ones(tf.shape(self.input_sequence)))\n",
    "        print(\"reconstruction_loss: {}\".format(self.reconstruction_loss))\n",
    "\n",
    "        self.reconstruction_loss_summary = tf.summary.scalar(\n",
    "            tensor=self.reconstruction_loss, name=\"reconstruction_loss\")\n",
    "\n",
    "\n",
    "    def train(self, sess):\n",
    "\n",
    "        writer = tf.summary.FileWriter(\n",
    "            logdir=\"/tmp/tensorflow_logs/\" + dt.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\", \n",
    "            graph=sess.graph)\n",
    "        \n",
    "        adversarial_training_optimizer = tf.train.AdamOptimizer()\n",
    "        adversarial_training_operation = adversarial_training_optimizer.minimize(\n",
    "            self.adversarial_loss)\n",
    "        \n",
    "        reconstruction_training_optimizer = tf.train.AdamOptimizer()\n",
    "        reconstruction_training_operation = reconstruction_training_optimizer.minimize(\n",
    "            self.reconstruction_loss - self.adversarial_loss)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        batch_size = 100\n",
    "        epoch_reporting_interval = 1\n",
    "        training_examples_fraction = 0.9\n",
    "        self.training_examples_size = int(training_examples_fraction * len(one_hot_labels))\n",
    "        training_epochs = 100\n",
    "        num_batches = self.training_examples_size // batch_size\n",
    "        print(\"Training - texts shape: {}; labels shape {}\"\n",
    "              .format(padded_sequences[:self.training_examples_size].shape, \n",
    "                      one_hot_labels[:self.training_examples_size].shape))\n",
    "\n",
    "        training_step = 1\n",
    "        for current_epoch in range(1, training_epochs + 1):\n",
    "            for batch_number in range(num_batches):\n",
    "                _, adv_loss, adv_loss_sum, _, rec_loss, rec_loss_sum = sess.run(\n",
    "                    [adversarial_training_operation, self.adversarial_loss, \n",
    "                     self.adversarial_loss_summary, \n",
    "                     reconstruction_training_operation, self.reconstruction_loss, \n",
    "                     self.reconstruction_loss_summary], \n",
    "                    feed_dict={\n",
    "                        self.input_sequence: padded_sequences[\n",
    "                            batch_number * batch_size : (batch_number + 1) * batch_size],\n",
    "                        self.input_label: one_hot_labels[\n",
    "                            batch_number * batch_size : (batch_number + 1) * batch_size]})\n",
    "                writer.add_summary(adv_loss_sum, training_step)\n",
    "                writer.add_summary(rec_loss_sum, training_step)\n",
    "                writer.flush()\n",
    "                training_step += 1\n",
    "\n",
    "            if (current_epoch % epoch_reporting_interval == 0):\n",
    "                print(\"Training epoch: {}; Adversarial Loss: {}; Reconstruction Loss: {}\"\n",
    "                      .format(current_epoch, adv_loss, rec_loss))\n",
    "        \n",
    "        writer.close()\n",
    "\n",
    "    def infer(self, sess):\n",
    "        \n",
    "        test_samples_size = len(one_hot_labels[self.training_examples_size:])\n",
    "        \n",
    "        training_predictions, generated_training_sequences = sess.run(\n",
    "            fetches=[self.label_prediction, self.generated_logits], \n",
    "            feed_dict={\n",
    "                self.input_sequence: padded_sequences[:test_samples_size], \n",
    "                self.input_label: one_hot_labels[:test_samples_size]\n",
    "            })\n",
    "\n",
    "        test_predictions, generated_test_sequences = sess.run(\n",
    "            fetches=[self.label_prediction, self.generated_logits],\n",
    "            feed_dict={\n",
    "                self.input_sequence: padded_sequences[-1 * test_samples_size:], \n",
    "                self.input_label: one_hot_labels[-1 * test_samples_size:]\n",
    "            })\n",
    "\n",
    "        return generated_training_sequences, generated_test_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "gan = GenerativeAdversarialNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "gan.train(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_training_sequences, generated_test_sequences = gan.infer(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_training_sequences.shape)\n",
    "print(generated_test_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_word_inverse_map = {v: k for k, v in text_tokenizer.word_index.items()}\n",
    "\n",
    "def generate_word(word_embedding):\n",
    "    return np.argmax(word_embedding)\n",
    "\n",
    "def generate_sentence(floating_index_sequence):\n",
    "    words_indices = map(generate_word, floating_index_sequence)\n",
    "    words = list(map(lambda x: index_word_inverse_map[x], words_indices))\n",
    "    \n",
    "    sentence = \" \".join(words)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_generated_sentences = list(map(generate_sentence, generated_test_sequences))\n",
    "print(test_generated_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyenv",
   "language": "python",
   "name": ".pyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
