{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from tensorflow.python.client import device_lib\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DEV_MODE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config_proto = tf.ConfigProto()\n",
    "config_proto.gpu_options.allow_growth=True  \n",
    "sess = tf.Session(config=config_proto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if DEV_MODE:\n",
    "    text_file_path = \"data/c50-articles-dev.txt\"\n",
    "    label_file_path = \"data/c50-labels-dev.txt\"\n",
    "    training_epochs = 3\n",
    "    VOCAB_SIZE = 1000\n",
    "else:\n",
    "    text_file_path = \"data/c50-articles.txt\"\n",
    "    label_file_path = \"data/c50-labels.txt\"\n",
    "    training_epochs = 50\n",
    "    VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion of texts into integer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=VOCAB_SIZE, filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "with open(text_file_path) as text_file:\n",
    "    text_tokenizer.fit_on_texts(text_file)\n",
    "    \n",
    "with open(text_file_path) as text_file:\n",
    "    integer_text_sequences = text_tokenizer.texts_to_sequences(text_file)\n",
    "\n",
    "text_sequence_lengths = np.asarray(\n",
    "    a=list(map(lambda x: len(x), integer_text_sequences)), dtype=np.int32)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = np.amax(text_sequence_lengths)\n",
    "\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "     integer_text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "print(\"text_sequence_lengths: {}\".format(text_sequence_lengths.shape))\n",
    "print(\"padded_sequences.shape: {}\".format(padded_sequences.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"MAX_SEQUENCE_LENGTH: {}\".format(MAX_SEQUENCE_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SOS_INDEX = text_tokenizer.word_index['<sos>']\n",
    "EOS_INDEX = text_tokenizer.word_index['<eos>']\n",
    "DATA_SIZE = padded_sequences.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# text_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion of labels to one-hot represenations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_tokenizer =  tf.keras.preprocessing.text.Tokenizer(lower=False)\n",
    "\n",
    "with open(label_file_path) as label_file:\n",
    "    label_tokenizer.fit_on_texts(label_file)\n",
    "\n",
    "with open(label_file_path) as label_file:\n",
    "    label_sequences = label_tokenizer.texts_to_sequences(label_file)\n",
    "\n",
    "NUM_LABELS = len(label_tokenizer.word_index)\n",
    "one_hot_labels = np.asarray(list(\n",
    "    map(lambda x: np.eye(NUM_LABELS, k=x[0])[0], label_sequences)))\n",
    "\n",
    "print(\"one_hot_labels.shape: {}\".format(one_hot_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_vector_path = \"word-embeddings/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding(word, model, dimensions):\n",
    "\n",
    "    vec_rep = np.zeros(dimensions)\n",
    "    if word in model:\n",
    "        vec_rep = model[word]\n",
    "    \n",
    "    return vec_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder_embedding_matrix = np.random.rand(VOCAB_SIZE + 1, EMBEDDING_SIZE).astype('float32')\n",
    "decoder_embedding_matrix = np.random.rand(VOCAB_SIZE + 1, EMBEDDING_SIZE).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not DEV_MODE:\n",
    "    # Google news pretrained vectors\n",
    "    wv_model_path = word_vector_path + \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "    wv_model_1 = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        wv_model_path, binary=True, unicode_errors='ignore')\n",
    "\n",
    "    i = 0\n",
    "    for key in text_tokenizer.word_index:\n",
    "        encoder_embedding_matrix[i] = get_word2vec_embedding(key, wv_model_1, 300)\n",
    "        decoder_embedding_matrix[i] = get_word2vec_embedding(key, wv_model_1, 300)\n",
    "        i += 1\n",
    "        if i >= VOCAB_SIZE:\n",
    "            break\n",
    "            \n",
    "    del wv_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(encoder_embedding_matrix.shape, decoder_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class GenerativeAdversarialNetwork():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.batch_size = 100\n",
    "        self.style_embedding_size = 512\n",
    "        self.content_embedding_size = 512\n",
    "    \n",
    "    def get_sentence_representation(self, embedded_sequence):\n",
    "\n",
    "        with tf.name_scope('sentence_representation'):\n",
    "            lstm_cell_fw = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell=tf.contrib.rnn.BasicLSTMCell(num_units=256),\n",
    "                input_keep_prob=0.75,\n",
    "                output_keep_prob=0.75,\n",
    "                state_keep_prob=0.75)\n",
    "\n",
    "            lstm_cell_bw = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell=tf.contrib.rnn.BasicLSTMCell(num_units=256),\n",
    "                input_keep_prob=0.75,\n",
    "                output_keep_prob=0.75,\n",
    "                state_keep_prob=0.75)\n",
    "\n",
    "            _, encoder_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw=lstm_cell_fw, cell_bw=lstm_cell_bw, \n",
    "                inputs=embedded_sequence,\n",
    "                sequence_length=self.sequence_lengths,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "            sentence_representation_dense = tf.concat(\n",
    "                values=[encoder_states[0].h, encoder_states[1].h], axis=1)\n",
    "\n",
    "            sentence_representation = tf.nn.dropout(\n",
    "                x=sentence_representation_dense, keep_prob=0.75)\n",
    "\n",
    "            return sentence_representation\n",
    "\n",
    "    def get_content_representation(self, sentence_representation):\n",
    "        \n",
    "        with tf.name_scope('content_representation'):\n",
    "            content_representation_dense = tf.layers.dense(\n",
    "                inputs=sentence_representation, units=self.content_embedding_size, \n",
    "                activation=tf.nn.relu, name=\"content_representation\")\n",
    "\n",
    "            content_representation = tf.nn.dropout(\n",
    "                x=content_representation_dense, keep_prob=0.75)\n",
    "\n",
    "            return content_representation\n",
    "\n",
    "    def get_style_representation(self, sentence_representation):\n",
    "        \n",
    "        with tf.name_scope('style_representation'):\n",
    "            style_representation_dense = tf.layers.dense(\n",
    "                inputs=sentence_representation, units=self.style_embedding_size, \n",
    "                activation=tf.nn.relu, name=\"style_representation\")\n",
    "\n",
    "            style_representation = tf.nn.dropout(\n",
    "                x=style_representation_dense, keep_prob=0.75)\n",
    "            \n",
    "            return style_representation\n",
    "\n",
    "    def get_label_prediction(self, content_representation):\n",
    "\n",
    "        with tf.name_scope('label_prediction'):\n",
    "            label_projection = tf.layers.dense(\n",
    "                inputs=content_representation, units=NUM_LABELS, \n",
    "                activation=tf.nn.relu, name=\"label_prediction\")\n",
    "\n",
    "            label_prediction = tf.nn.softmax(label_projection)\n",
    "\n",
    "            return label_prediction\n",
    "    \n",
    "    \n",
    "    def generate_output_sequence(self, embedded_sequence, generative_embedding, \n",
    "                                 decoder_embeddings):\n",
    "        \n",
    "        \n",
    "        def get_training_decoder_output():\n",
    "\n",
    "            decoder_cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell=tf.nn.rnn_cell.BasicLSTMCell(\n",
    "                    num_units=256, state_is_tuple=False),\n",
    "                input_keep_prob=0.75,\n",
    "                output_keep_prob=0.75,\n",
    "                state_keep_prob=0.75)\n",
    "            \n",
    "            training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                inputs=embedded_sequence, \n",
    "                sequence_length=tf.fill([self.batch_size], MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "            training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell=decoder_cell, helper=training_helper, \n",
    "                initial_state=generative_embedding,\n",
    "                output_layer=tf.layers.Dense(\n",
    "                    units=VOCAB_SIZE, activation=tf.nn.relu))\n",
    "\n",
    "            # Dynamic decoding\n",
    "            training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder=training_decoder, impute_finished=True,\n",
    "                maximum_iterations=MAX_SEQUENCE_LENGTH)\n",
    "            \n",
    "            return training_decoder_output\n",
    "\n",
    "        def get_inference_decoder_output():\n",
    "\n",
    "            decoder_cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell=tf.nn.rnn_cell.BasicLSTMCell(\n",
    "                    num_units=256, state_is_tuple=False),\n",
    "                input_keep_prob=0.75,\n",
    "                output_keep_prob=0.75,\n",
    "                state_keep_prob=0.75)\n",
    "\n",
    "            greedy_embedding_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                embedding=decoder_embeddings, \n",
    "                start_tokens=tf.fill([self.batch_size], SOS_INDEX), \n",
    "                end_token=EOS_INDEX)\n",
    "\n",
    "            inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell=decoder_cell, helper=greedy_embedding_helper, \n",
    "                initial_state=generative_embedding,\n",
    "                output_layer=tf.layers.Dense(\n",
    "                    units=VOCAB_SIZE, activation=tf.nn.relu))\n",
    "\n",
    "            # Dynamic decoding\n",
    "            inference_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder=inference_decoder, impute_finished=True, \n",
    "                maximum_iterations=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "            return inference_decoder_output\n",
    "        \n",
    "        decoder_output = tf.cond(\n",
    "            pred=self.training_phase, \n",
    "            true_fn=get_training_decoder_output, \n",
    "            false_fn=get_inference_decoder_output,\n",
    "            name=\"training_inference_conditional\")\n",
    "\n",
    "        return decoder_output.rnn_output\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        self.input_sequence = tf.placeholder(\n",
    "            dtype=tf.int32, shape=[self.batch_size, MAX_SEQUENCE_LENGTH], \n",
    "            name=\"input_sequence\")\n",
    "        print(\"input_sequence: {}\".format(self.input_sequence))\n",
    "\n",
    "        self.input_label = tf.placeholder(\n",
    "            dtype=tf.float32, shape=[self.batch_size, NUM_LABELS], \n",
    "            name=\"input_label\")\n",
    "        print(\"input_label: {}\".format(self.input_label))\n",
    "\n",
    "        self.sequence_lengths = tf.placeholder(\n",
    "            dtype=tf.int32, shape=[self.batch_size], \n",
    "            name=\"sequence_lengths\")\n",
    "        print(\"sequence_lengths: {}\".format(self.sequence_lengths))\n",
    "\n",
    "        self.training_phase = tf.placeholder(\n",
    "            dtype=tf.bool, name=\"training_phase\")\n",
    "        print(\"training_phase: {}\".format(self.training_phase))\n",
    "\n",
    "        # word embeddings matrix\n",
    "        encoder_embeddings = tf.get_variable(\n",
    "            initializer=encoder_embedding_matrix, dtype=tf.float32,\n",
    "            name=\"encoder_embeddings\")\n",
    "        print(\"encoder_embeddings: {}\".format(encoder_embeddings))\n",
    "\n",
    "        encoder_embedded_sequence = tf.nn.embedding_lookup(\n",
    "            params=encoder_embeddings, ids=self.input_sequence, \n",
    "            name=\"encoder_embedded_sequence\")\n",
    "        print(\"encoder_embedded_sequence: {}\".format(encoder_embedded_sequence))\n",
    "\n",
    "        # get sentence representation\n",
    "        sentence_representation = self.get_sentence_representation(\n",
    "            encoder_embedded_sequence)\n",
    "        print(\"sentence_representation: {}\".format(sentence_representation))\n",
    "\n",
    "        # get content representation\n",
    "        content_representation = self.get_content_representation(\n",
    "            sentence_representation)\n",
    "        print(\"content_representation: {}\".format(content_representation))\n",
    "\n",
    "        # get style representation\n",
    "        self.style_representation = self.get_style_representation(\n",
    "            sentence_representation)\n",
    "        print(\"style_representation: {}\".format(self.style_representation))\n",
    "\n",
    "        # use content representation to predict a label\n",
    "        self.label_prediction = self.get_label_prediction(\n",
    "            content_representation)\n",
    "        print(\"label_prediction: {}\".format(self.label_prediction))\n",
    "\n",
    "        self.adversarial_loss = tf.losses.softmax_cross_entropy(\n",
    "            onehot_labels=self.input_label, logits=self.label_prediction)\n",
    "        print(\"adversarial_loss: {}\".format(self.adversarial_loss))\n",
    "        \n",
    "        # generate new sentence\n",
    "        with tf.name_scope('generative_embedding'):\n",
    "            generative_embedding_concatenated = tf.concat(\n",
    "                values=[self.style_representation, content_representation], axis=1)\n",
    "            \n",
    "            generative_embedding_dense = tf.layers.dense(\n",
    "                inputs=generative_embedding_concatenated, \n",
    "                units=512, \n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "            generative_embedding = tf.nn.dropout(\n",
    "                x=generative_embedding_dense, keep_prob=0.75)\n",
    "            print(\"generative_embedding: {}\".format(generative_embedding))\n",
    "        \n",
    "        decoder_embeddings = tf.get_variable(\n",
    "            initializer=decoder_embedding_matrix, dtype=tf.float32,\n",
    "            name=\"decoder_embeddings\")\n",
    "        print(\"decoder_embeddings: {}\".format(decoder_embeddings))\n",
    "        \n",
    "        decoder_embedded_sequence = tf.nn.embedding_lookup(\n",
    "            params=decoder_embeddings, ids=self.input_sequence, \n",
    "            name=\"decoder_embedded_sequence\")\n",
    "        print(\"decoder_embedded_sequence: {}\".format(decoder_embedded_sequence))\n",
    "\n",
    "        with tf.name_scope('sequence_prediction'):\n",
    "            self.sequence_prediction = \\\n",
    "                self.generate_output_sequence(\n",
    "                    decoder_embedded_sequence, generative_embedding, decoder_embeddings)\n",
    "\n",
    "        with tf.name_scope('reconstruction_loss'):\n",
    "            output_sequence_mask = tf.sequence_mask(\n",
    "                lengths=self.sequence_lengths, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                dtype=tf.float32)\n",
    "\n",
    "            self.reconstruction_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                logits=self.sequence_prediction, targets=self.input_sequence, \n",
    "                weights=output_sequence_mask)\n",
    "\n",
    "            print(\"reconstruction_loss: {}\".format(self.reconstruction_loss))\n",
    "        \n",
    "        # loss summaries for tensorboard logging\n",
    "        self.adversarial_loss_summary = tf.summary.scalar(\n",
    "            tensor=self.adversarial_loss, name=\"adversarial_loss_summary\")\n",
    "\n",
    "        self.reconstruction_loss_summary = tf.summary.scalar(\n",
    "            tensor=self.reconstruction_loss, name=\"reconstruction_loss_summary\")\n",
    "        \n",
    "    def get_batch_indices(self, offset, batch_size, batch_number, data_limit):\n",
    "        \n",
    "        start_index = offset + (batch_number * batch_size)\n",
    "        end_index = offset + ((batch_number + 1) * batch_size)\n",
    "\n",
    "        end_index = data_limit if end_index > data_limit else end_index\n",
    "\n",
    "        return (start_index, end_index)\n",
    "\n",
    "\n",
    "    def run_batch(self, start_index, end_index, training_phase, fetches):\n",
    "        \n",
    "        ops = sess.run(\n",
    "            fetches=fetches, \n",
    "            feed_dict={\n",
    "                self.input_sequence: padded_sequences[\n",
    "                    start_index : end_index],\n",
    "                self.input_label: one_hot_labels[\n",
    "                    start_index : end_index],\n",
    "                self.sequence_lengths: text_sequence_lengths[\n",
    "                    start_index : end_index],\n",
    "                self.training_phase: training_phase\n",
    "            })\n",
    "        \n",
    "        return ops\n",
    "        \n",
    "    def train(self, sess):\n",
    "\n",
    "        writer = tf.summary.FileWriter(\n",
    "            logdir=\"/tmp/tensorflow_logs/\" + dt.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\", \n",
    "            graph=sess.graph)\n",
    "        \n",
    "        adversarial_training_optimizer = tf.train.AdamOptimizer()\n",
    "        adversarial_training_operation = adversarial_training_optimizer.minimize(\n",
    "            self.adversarial_loss)\n",
    "        \n",
    "        reconstruction_training_optimizer = tf.train.AdamOptimizer()\n",
    "        reconstruction_training_operation = reconstruction_training_optimizer.minimize(\n",
    "            self.reconstruction_loss - self.adversarial_loss)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        epoch_reporting_interval = 1\n",
    "        self.training_examples_size = DATA_SIZE\n",
    "        num_batches = self.training_examples_size // self.batch_size\n",
    "        print(\"Training - texts shape: {}; labels shape {}\"\n",
    "              .format(padded_sequences[:self.training_examples_size].shape, \n",
    "                      one_hot_labels[:self.training_examples_size].shape))\n",
    "\n",
    "        for current_epoch in range(1, training_epochs + 1):\n",
    "            self.all_style_representations = list()\n",
    "            for batch_number in range(num_batches):\n",
    "                \n",
    "                (start_index, end_index) = self.get_batch_indices(\n",
    "                    offset=0, batch_size=self.batch_size, \n",
    "                    batch_number=batch_number, data_limit=DATA_SIZE)\n",
    "                \n",
    "                fetches = [adversarial_training_operation, \n",
    "                           self.adversarial_loss, \n",
    "                           self.adversarial_loss_summary, \n",
    "                           reconstruction_training_operation, \n",
    "                           self.reconstruction_loss, \n",
    "                           self.reconstruction_loss_summary, \n",
    "                           self.style_representation]\n",
    "                \n",
    "                _, adv_loss, adv_loss_sum, _, rec_loss, \\\n",
    "                rec_loss_sum, style_embeddings = self.run_batch(\n",
    "                    start_index=start_index, end_index=end_index, \n",
    "                    training_phase=True, \n",
    "                    fetches=fetches)\n",
    "                \n",
    "                self.all_style_representations.extend(style_embeddings)\n",
    "                    \n",
    "            writer.add_summary(adv_loss_sum, current_epoch)\n",
    "            writer.add_summary(rec_loss_sum, current_epoch)\n",
    "            writer.flush()\n",
    "\n",
    "            if (current_epoch % epoch_reporting_interval == 0):\n",
    "                print(\"Training epoch: {}; Reconstruction loss: {}; Adversarial loss {}\"\\\n",
    "                      .format(current_epoch, rec_loss, adv_loss))\n",
    "        \n",
    "        writer.close()\n",
    "\n",
    "    def infer(self, sess, offset, samples_size):\n",
    "        \n",
    "        generated_sequences = list()\n",
    "        num_batches = samples_size // self.batch_size\n",
    "        \n",
    "        for batch_number in range(num_batches + 1):\n",
    "\n",
    "            (start_index, end_index) = self.get_batch_indices(\n",
    "                offset=offset, batch_size=self.batch_size, \n",
    "                batch_number=batch_number, data_limit=(offset + samples_size))\n",
    "\n",
    "            if start_index == end_index:\n",
    "                break\n",
    "            \n",
    "            generated_sequences_batch = self.run_batch(\n",
    "                start_index=start_index, end_index=end_index,\n",
    "                training_phase=False, fetches=self.sequence_prediction)\n",
    "            \n",
    "            generated_sequences.extend(generated_sequences_batch)\n",
    "\n",
    "        return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gan = GenerativeAdversarialNetwork()\n",
    "gan.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gan.train(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inference_set_size = 1 * gan.batch_size\n",
    "offset = random.randint(0, (DATA_SIZE - 1) - inference_set_size)\n",
    "print(\"range: {}-{}\".format(offset, (offset + inference_set_size)))\n",
    "\n",
    "actual_sequences = integer_text_sequences[offset : (offset + inference_set_size)]\n",
    "# print(actual_sequences)\n",
    "generated_sequences = gan.infer(sess, offset, inference_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index_word_inverse_map = {v: k for k, v in text_tokenizer.word_index.items()}\n",
    "bleu_score_weights = {\n",
    "    1: (1.0, 0.0, 0.0, 0.0),\n",
    "    2: (0.5, 0.5, 0.0, 0.0),\n",
    "    3: (0.34, 0.33, 0.33, 0.0),\n",
    "    4: (0.25, 0.25, 0.25, 0.25),\n",
    "}\n",
    "\n",
    "def generate_word(word_embedding):\n",
    "    return np.argmax(word_embedding)\n",
    "\n",
    "def generate_sentence_from_indices(index_sequence):\n",
    "    words = list(map(lambda x: index_word_inverse_map[x], index_sequence))\n",
    "    return words\n",
    "\n",
    "def generate_sentence_from_logits(floating_index_sequence):\n",
    "    word_indices = map(generate_word, floating_index_sequence)\n",
    "    word_indices = list(filter(lambda x: x > 0, word_indices))    \n",
    "    words = list(map(lambda x: index_word_inverse_map[x], word_indices))\n",
    "    return words\n",
    "\n",
    "def get_corpus_bleu_scores(actual_word_lists, generated_word_lists):\n",
    "    bleu_scores = dict()\n",
    "    for i in range(1, 5):\n",
    "        bleu_scores[i] = corpus_bleu(\n",
    "            list_of_references=actual_word_lists, \n",
    "            hypotheses=generated_word_lists,\n",
    "            weights=bleu_score_weights[i])\n",
    "        \n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actual_word_lists = list(map(generate_sentence_from_indices, actual_sequences))\n",
    "generated_word_lists = list(map(generate_sentence_from_logits, generated_sequences))\n",
    "# print(actual_word_lists)\n",
    "\n",
    "bleu_scores = get_corpus_bleu_scores(\n",
    "    list(map(lambda x: [x], actual_word_lists)), \n",
    "    generated_word_lists)\n",
    "print(\"bleu_scores: {}\".format(bleu_scores))\n",
    "\n",
    "actual_sentences = list()\n",
    "generated_sentences = list()\n",
    "print(list(map(lambda x: len(list(x)), generated_word_lists)))\n",
    "for i in range(100):\n",
    "    print(len(generated_word_lists[i]))\n",
    "    actual_sentence = \" \".join(actual_word_lists[i][1:])\n",
    "#     print(\"actual_sentence: {}\".format(actual_sentence))\n",
    "    actual_sentences.append(actual_sentence)\n",
    "    \n",
    "    generated_sentence = \" \".join(generated_word_lists[i])\n",
    "    print(\"generated_sentence: {}\".format(generated_sentence))\n",
    "    generated_sentences.append(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not DEV_MODE:\n",
    "    output_file_path = \"output/generated_sentences_{}.txt\".format(dt.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        for disjoint_sentence in generated_sentences:\n",
    "            output_file.writelines(generated_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "style_embeddings = np.asarray(gan.all_style_representations)\n",
    "print(\"style_embeddings_shape: {}\".format(style_embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_author_embeddings = dict()\n",
    "for i in range(DATA_SIZE):\n",
    "    author_label = label_sequences[i][0]\n",
    "    if author_label not in all_author_embeddings:\n",
    "        all_author_embeddings[author_label] = list()\n",
    "    all_author_embeddings[author_label].append(style_embeddings[i])\n",
    "\n",
    "average_author_embeddings = dict()\n",
    "for author_label in all_author_embeddings:\n",
    "    average_author_embeddings[author_label] = np.mean(all_author_embeddings[author_label], axis=0)\n",
    "\n",
    "# print(average_author_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyenv",
   "language": "python",
   "name": ".pyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
